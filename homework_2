MODULE = 'C:\Python27\Lib\site-packages\pattern-2.6'
import sys
import re
from itertools import islice, tee
if MODULE not in sys.path: sys.path.append(MODULE)
from pattern.en import parsetree
from pattern.web import Wikipedia, plaintext

class WikiParser:
    def __init__(self):
        pass
    
    def get_articles(self, start, depth, max_count):
        list_of_strings = []
        article = Wikipedia().article(start).plaintext()
        list_of_strings.append(self.plain_text(article))
        if depth > 1:
            for link in Wikipedia().article(start).links:
                if Wikipedia().article(link).language is 'en':
                    list_of_strings.append(self.plain_text(link))
        return list_of_strings
    
    def plain_text(self, article):
        corrected = article.lower()
        corrected = re.sub('\n', ' ', corrected)
        corrected = re.sub('[.,?!:;*\'"]', ' ', corrected)
        while re.search('  ', corrected) != None:
            corrected = re.sub('  ', ' ', corrected)
        return corrected

class TextStatistics:
    def __init___(self, articles):
        pass

    def clean_text(text):
        text = re.sub('\n', ' ', text)
        text = re.sub('[.,?!:;*\'-_"0123456789\(\)\]\[]', '', text)
        while re.search('  ', text) != None:
            text = re.sub('  ', ' ', text)
        return text
    
    def make_ngrams(text, big_dict):
        ngrams = zip(*(islice(seq, index, None) for index, seq in enumerate(tee(text, 3))))
        ngrams = [''.join(x) for x in ngrams]
        for gram in ngrams:
            if gram in big_dict:
                big_dict[gram] += 1
            else:
                big_dict[gram] = 0
        return big_dict

    def count_words(text, big_dict):
        text = text.split(' ' )
        for word in text:
            if word in big_dict:
                big_dict[word] += 1
            else:
                big_dict[word] = 0
        return big_dict

    def get_top_3grams(self, n):
        list_of_3grams_in_descending_order_by_freq = []
        list_of_their_corresponding_freq = []
        big_dict = {}
        for text in n:
            make_ngrams(clean_text(text), big_dict)
        for ngram in sorted(big_dict, key=lambda n: big_dict[n], reverse=True):
            list_of_3grams_in_descending_order_by_freq.append(ngram)
            list_of_their_corresponding_freq.append(big_dict[ngram])
        return (list_of_3grams_in_descending_order_by_freq, list_of_their_corresponding_freq)

    def get_top_words(self, n):
        big_dict = {}
        list_of_words_in_descending_order_by_freq = []
        list_of_their_corresponding_freq = []
        for text in n:
            count_words(clean_text(text), big_dict)
        for word in sorted(big_dict, key=lambda n: big_dict[n], reverse=True):
            list_of_words_in_descending_order_by_freq.append(word)
            list_of_their_corresponding_freq.append(big_dict[word])
        return (list_of_words_in_descending_order_by_freq, list_of_their_corresponding_freq)
    
class Experiment:
    def __init___(self, articles):
        pass

    def show_results(self):
        start = 'Natural language processing'
        articles = WikiParser().get_articles(start, 2, max_count=20)
        top_20_ngr = TextStatistics().get_top_3grams(articles)[:20]
        top_20_words = TextStatistics().get_top_3grams(articles)[:20]
        print 'Top-20 3grams in article', start, 'and all the artickes it refers to:', top_20_ngr
        print 'Top-20 words in article', start, 'and all the artickes it refers to:', top_20_words
        start_only = WikiParser().get_articles(start, 1, max_count=5)
        top_5_ngr_in_start = TextStatistics().get_top_3grams(articles)[:5]
        top_5_words_in_start = TextStatistics().get_top_3grams(articles)[:5]
        print 'Top-5 3grams in article', start, top_5_ngr_in_start
        print 'Top-5 words in article', start, top_5_words_in_start
        
Experiment().show_results()
